#!/bin/bash
set -e

#--------------------------------------
: ${QUEUE:="prod"}
: ${NEKRS_GPU_MPI:=0}
: ${NEKRS_BACKEND:="dpcpp"}
: ${RANKS_FOR_BUILD:=12}
: ${SIM_RANKS_PER_NODE:=6}
: ${TRAIN_RANKS_PER_NODE:=6}
: ${DEPLOYMENT:="colocated"}
: ${SIM_NODES:=1}
: ${TRAIN_NODES:=1}
: ${SIM_CPU_BIND_LIST:="1:8:16:24:32:40"}
: ${TRAIN_CPU_BIND_LIST:="53:60:68:76:84:92"}
: ${INFERENCE_CPU_BIND_LIST:="1:8:16:24:32:40:53:60:68:76:84:92"}
: ${OCCA_DPCPP_COMPILER_FLAGS:="-O3 -fsycl -fsycl-targets=intel_gpu_pvc -ftarget-register-alloc-mode=pvc:auto -fma"}
: ${ONEAPI_SDK:=""}
: ${FRAMEWORKS_MODULE:="frameworks"}
: ${VENV_PATH:=""}
#--------------------------------------

#--------------------------------------
# Copy the .par file
cp ${1}.par.safe ${1}.par

#--------------------------------------
# Update the .box file to increase the mesh size linearly with the number of nodes
SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
NZ=$(( 26*SIM_PROCS ))
LZ=$(( 1*SIM_PROCS ))
cp ${1}_safe.box ${1}.box
sed -i "s/NZ/${NZ}/g" ${1}.box
sed -i "s/LZ/${LZ}/g" ${1}.box

#--------------------------------------
# Run genbox from Nek5000 to generate the .re2 mesh file
if [ ! -d $NEKRS_HOME/Nek5000 ]; then
  CWD=$PWD
  cd $NEKRS_HOME
  git clone https://github.com/rickybalin/Nek5000.git
  cd Nek5000/tools
  FC=`which gfortran` ./maketools genbox
  cd $CWD
fi
$NEKRS_HOME/Nek5000/bin/genbox ${1}.box ${1}

#--------------------------------------
# Setup the case
source $NEKRS_HOME/bin/nrsqsub_utils
setup $# 1
chk_case $TOTAL_RANKS

#--------------------------------------
# Generate the affinity scripts
SAFF_FILE=affinity_nrs.sh
TAFF_FILE=affinity_ml.sh
if [ ${ZE_FLAT_DEVICE_HIERARCHY} == "COMPOSITE" ] || [[ -z "${ZE_FLAT_DEVICE_HIERARCHY}" ]]; then
  echo "#!/bin/bash" > $SAFF_FILE
  echo "num_gpus=\$1" >> $SAFF_FILE
  echo "shift" >> $SAFF_FILE
  echo "num_tiles=2" >> $SAFF_FILE
  echo "gpu_id=\$(((PALS_LOCAL_RANKID / \${num_tiles}) % \${num_gpus}))" >> $SAFF_FILE
  echo "tile_id=\$((PALS_LOCAL_RANKID % \${num_tiles}))" >> $SAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $SAFF_FILE
  echo "exec \"\$@\"" >> $SAFF_FILE

  echo "#!/bin/bash" > $TAFF_FILE
  echo "num_gpus=\$1" >> $TAFF_FILE
  echo "offset=\$2" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "num_tiles=2" >> $TAFF_FILE
  echo "gpu_id=\$(((PALS_LOCAL_RANKID / \${num_tiles}) % \${num_gpus} + (offset / num_tiles) ))" >> $TAFF_FILE
  echo "tile_id=\$((PALS_LOCAL_RANKID % \${num_tiles}))" >> $TAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $TAFF_FILE
  echo "exec \"\$@\"" >> $TAFF_FILE
elif [ ${ZE_FLAT_DEVICE_HIERARCHY} == "FLAT" ]; then
  echo "#!/bin/bash" > $SAFF_FILE
  echo "num_gpus=\$1" >> $SAFF_FILE
  echo "shift" >> $SAFF_FILE
  echo "gpu_id=\$((PALS_LOCAL_RANKID % \${num_gpus} ))" >> $SAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id" >> $SAFF_FILE
  echo "exec \"\$@\"" >> $SAFF_FILE

  echo "#!/bin/bash" > $TAFF_FILE
  echo "num_gpus=\$1" >> $TAFF_FILE
  echo "offset=\$2" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "gpu_id=\$((PALS_LOCAL_RANKID % \${num_gpus} + \${offset} ))" >> $TAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id" >> $TAFF_FILE
  echo "exec \"\$@\"" >> $TAFF_FILE
fi
chmod u+x $SAFF_FILE $TAFF_FILE

#--------------------------------------
# Generate the workflow config script
CFILE=config.yaml
echo "# Workflow config" > $CFILE
echo "scheduler: pbs" >> $CFILE
echo "deployment: \"${DEPLOYMENT}\"" >> $CFILE
echo "" >> $CFILE
echo "# Run config" >> $CFILE
echo "run_args:" >> $CFILE
echo "    nodes: ${qnodes}" >> $CFILE
if [ ${DEPLOYMENT} == "colocated"  ]; then
  SIM_NODES=$nodes
  TRAIN_NODES=$nodes
  SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
  TRAIN_PROCS=$(( TRAIN_NODES * TRAIN_RANKS_PER_NODE ))
  echo "    sim_nodes: ${SIM_NODES}" >> $CFILE
  echo "    ml_nodes: ${SIM_NODES}" >> $CFILE
  echo "    simprocs: ${SIM_PROCS}" >> $CFILE
  echo "    simprocs_pn: ${SIM_RANKS_PER_NODE}" >> $CFILE
  echo "    mlprocs: ${TRAIN_PROCS}" >> $CFILE
  echo "    mlprocs_pn: ${TRAIN_RANKS_PER_NODE}" >> $CFILE
  echo "    sim_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
  echo "    ml_cpu_bind: \"list:${TRAIN_CPU_BIND_LIST}\"" >> $CFILE
elif [ ${DEPLOYMENT} == "clustered"  ]; then
  SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
  TRAIN_PROCS=$(( TRAIN_NODES * TRAIN_RANKS_PER_NODE ))
  echo "    sim_nodes: ${SIM_NODES}" >> $CFILE
  echo "    ml_nodes: ${TRAIN_NODES}" >> $CFILE
  echo "    simprocs: ${SIM_PROCS}" >> $CFILE
  echo "    simprocs_pn: ${SIM_RANKS_PER_NODE}" >> $CFILE
  echo "    mlprocs: ${TRAIN_PROCS}" >> $CFILE
  echo "    mlprocs_pn: ${TRAIN_RANKS_PER_NODE}" >> $CFILE
  echo "    sim_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
  echo "    ml_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
fi
echo "" >> $CFILE
echo "# Simulation config" >> $CFILE
echo "sim:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/bin/nekrs\"" >> $CFILE
echo "    arguments: \"--setup ${case}.par --backend ${NEKRS_BACKEND} --device-id 0\"" >> $CFILE
echo "    affinity: \"./${SAFF_FILE}\"" >> $CFILE
echo "" >> $CFILE
echo "# Trainer config" >> $CFILE
echo "train:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/3rd_party/dist-gnn/main.py\"" >> $CFILE
#echo "    affinity: \"./${TAFF_FILE}\"" >> $CFILE
echo "    affinity: \"\"" >> $CFILE
if [ ${DEPLOYMENT} == "colocated"  ]; then
  echo "    arguments: \"device_skip=${SIM_RANKS_PER_NODE} precision=bf16 phase1_steps=200 hidden_channels=256 n_mlp_hidden_layers=2 n_messagePassing_layers=8 backend=xccl halo_swap_mode=all_to_all_opt online=True client.backend=adios client.adios_transport=RDMA online_update_freq=50 time_dependency=time_dependent verbose=True\"" >> $CFILE
elif [ ${DEPLOYMENT} == "clustered"  ]; then
  echo "    arguments: \"precision=bf16 phase1_steps=500 hidden_channels=256 n_mlp_hidden_layers=2 n_messagePassing_layers=8 backend=xccl halo_swap_mode=all_to_all_opt online=True client.backend=adios online_update_freq=50 time_dependency=time_dependent verbose=True\"" >> $CFILE
fi
echo "" >> $CFILE
echo "# Inference config" >> $CFILE
echo "inference:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/3rd_party/dist-gnn/inference.py\"" >> $CFILE
#echo "    affinity: \"./${TAFF_FILE}\"" >> $CFILE
echo "    affinity: \"\"" >> $CFILE
echo "    arguments: \"model_task=inference rollout_steps=200 hidden_channels=256 n_mlp_hidden_layers=2 n_messagePassing_layers=8 backend=xccl halo_swap_mode=all_to_all_opt online=True client.backend=adios time_dependency=time_dependent verbose=True\"" >> $CFILE

#--------------------------------------
# Update the .par file with the ML options
PFILE=${case}.par
echo "" >> $PFILE
echo "[ML]" >> $PFILE
echo "gnnPolynomialOrder = 2" >> $PFILE
echo "adiosEngine = SST" >> $PFILE
echo "adiosTransport = RDMA" >> $PFILE
echo "adiosStream = async" >> $PFILE

#--------------------------------------
# Generate the run script
RFILE=run.sh
echo "#!/bin/bash -l" > $RFILE
echo "##PBS -S /bin/bash" >> $RFILE
echo "##PBS -N nekrs-ml" >> $RFILE
echo "##PBS -l walltime=${TIME}:00" >> $RFILE
echo "##PBS -l select=$NODES" >> $RFILE
echo "##PBS -l filesystems=home:flare" >> $RFILE
echo "##PBS -k doe" >> $RFILE
echo "##PBS -j oe" >> $RFILE
echo "##PBS -A $PROJ_ID" >> $RFILE
echo "##PBS -q $QUEUE" >> $RFILE
echo "#cd \$PBS_O_WORKDIR" >> $RFILE

echo -e "\nexport TZ='/usr/share/zoneinfo/US/Central'" >> $RFILE

echo -e "\necho Jobid: \$PBS_JOBID" >>$RFILE
echo "echo Running on host \`hostname\`" >>$RFILE
echo "echo Running on nodes \`cat \$PBS_NODEFILE\`" >>$RFILE

echo "module load ${FRAMEWORKS_MODULE}" >> $RFILE
echo "source ${VENV_PATH}" >> $RFILE
echo "module list" >> $RFILE

echo -e "\nexport NEKRS_HOME=$NEKRS_HOME" >>$RFILE
echo "export OCCA_DPCPP_COMPILER_FLAGS=\"$OCCA_DPCPP_COMPILER_FLAGS\"" >> $RFILE
echo "export FI_CXI_RX_MATCH_MODE=hybrid" >> $RFILE # required by parRSB

# Temporary workaround while waiting on bugfix in runtime
echo "export UR_L0_USE_COPY_ENGINE=0" >> $RFILE

echo -e "\nexport CCL_ALLTOALLV_MONOLITHIC_KERNEL=0" >> $RFILE

echo -e "\nexport PYTHONPATH=\$PYTHONPATH:${NEKRS_HOME}/lib/python3.10/site-packages" >> $RFILE
echo "#export SstVerbose=1" >> $RFILE
echo "export OMP_PROC_BIND=spread" >> $RFILE
echo "export OMP_PLACES=threads" >> $RFILE
echo "if ls *.sst 1> /dev/null 2>&1" >> $RFILE
echo "then" >> $RFILE 
echo "    echo Cleaning up old .sst files" >> $RFILE
echo "    rm *.sst" >> $RFILE
echo "fi" >> $RFILE
echo "if ls *.bp 1> /dev/null 2>&1" >> $RFILE
echo "then" >> $RFILE
echo "    echo Cleaning up old .bp files" >> $RFILE
echo "    rm -r ./*.bp" >> $RFILE
echo "fi" >> $RFILE

echo -e "\n# precompilation" >> $RFILE
echo "if [ ! -d .cache ]; then" >> $RFILE
echo "  echo Cleaning up the cache" >> $RFILE
echo "  rm -r .cache" >> $RFILE
echo "fi" >> $RFILE
CMD_build="mpiexec -n ${RANKS_FOR_BUILD} -ppn ${RANKS_FOR_BUILD} --cpu-bind list:1:8:16:24:32:40:53:60:68:76:84:92 -- ./${SAFF_FILE} ${RANKS_FOR_BUILD} $bin --setup ${case} --backend ${NEKRS_BACKEND} --device-id 0 $extra_args --build-only ${RANKS_FOR_BUILD}"
add_build_CMD "$RFILE" "$CMD_build" "$TOTAL_RANKS"

echo -e "\n# actual run" >>$RFILE
echo "python driver.py" >> $RFILE
chmod u+x $RFILE

