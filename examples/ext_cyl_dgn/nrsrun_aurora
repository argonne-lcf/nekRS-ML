#!/bin/bash
set -e

#--------------------------------------
: ${QUEUE:="lustre_scaling"}
: ${NEKRS_GPU_MPI:=0}
: ${NEKRS_BACKEND:="dpcpp"}
: ${RANKS_PER_NODE:=12}
: ${CPU_BIND_LIST:="1:8:16:24:32:40:53:60:68:76:84:92"}
: ${OCCA_DPCPP_COMPILER_FLAGS:="-O3 -fsycl -fsycl-targets=intel_gpu_pvc -ftarget-register-alloc-mode=pvc:auto -fma"}
: ${ONEAPI_SDK:=""}
: ${FRAMEWORKS_MODULE:="frameworks"}
: ${VENV_PATH:=""}
#--------------------------------------

source $NEKRS_HOME/bin/nrsqsub_utils
setup $# 1

TOTAL_RANKS=$(( nodes * RANKS_PER_NODE ))
chk_case $TOTAL_RANKS

#--------------------------------------
# Generate the run script
RFILE=run.sh
echo "#!/bin/bash" > $RFILE
echo "set -e" >> $RFILE
echo "export TZ='/usr/share/zoneinfo/US/Central'" >> $RFILE

echo -e "\necho Jobid: \$PBS_JOBID" >>$RFILE
echo "echo Running on host \`hostname\`" >>$RFILE
echo "echo Running on nodes \`cat \$PBS_NODEFILE\`" >>$RFILE

echo "module restore" >> $RFILE
echo "module load ${FRAMEWORKS_MODULE}" >> $RFILE
echo "source ${VENV_PATH}" >> $RFILE
echo "module list" >> $RFILE

echo -e "\nexport NEKRS_HOME=$NEKRS_HOME" >>$RFILE
#echo "export NEKRS_GPU_MPI=$NEKRS_GPU_MPI" >>$RFILE
#echo "export MPICH_GPU_SUPPORT_ENABLED=$NEKRS_GPU_MPI" >> $RFILE
echo "export OCCA_DPCPP_COMPILER_FLAGS=\"$OCCA_DPCPP_COMPILER_FLAGS\"" >> $RFILE
echo "export FI_CXI_RX_MATCH_MODE=hybrid" >> $RFILE # required by parRSB

# Temporary workaround while waiting on bugfix in runtime
echo "export UR_L0_USE_COPY_ENGINE=0" >> $RFILE

echo -e "\n# Precompilation" >>$RFILE
CMD_build="mpiexec -n ${RANKS_PER_NODE} -ppn ${RANKS_PER_NODE} --cpu-bind list:${CPU_BIND_LIST} -- $bin --setup ${case} --backend ${NEKRS_BACKEND} $extra_args --build-only ${RANKS_PER_NODE}"
add_build_CMD "$RFILE" "$CMD_build" "$TOTAL_RANKS"

echo -e "\n# Run nekRS" >>$RFILE
echo "mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} -- $NEKRS_HOME/bin/nekrs --setup ${case} --backend ${NEKRS_BACKEND} 2>&1 | tee nekrs.log" >> $RFILE

echo -e "\n# Generate the halo_info, edge_weights and node_degree files" >>$RFILE
echo "mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/create_halo_info_par.py --POLY 3 --PATH ./gnn_outputs_poly_3" >> $RFILE

echo -e "\n# Train model" >>$RFILE
echo "head_node=\`head -1 \$PBS_NODEFILE | cut -d'.' -f1\`" >>$RFILE
echo "mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/main.py batch_size=4 mlp_hidden_channels=32 n_mlp_hidden_layers=5 n_messagePassing_layers=4 input_node_features=2 halo_swap_mode=none consistency=False phase1_steps=300 phase2_steps=700 lr_phase23=0.00001 ckptfreq=100 backend=xccl gnn_outputs_path=${PWD}/gnn_outputs_poly_3 verbose=True postprocess=False 2>&1 | tee train.log" >> $RFILE

echo -e "\n# Perform inference and generate files for visualization" >>$RFILE
echo "mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/inference.py model_task=inference num_gen_samples=1 mlp_hidden_channels=32 n_mlp_hidden_layers=5 n_messagePassing_layers=4 input_node_features=2 halo_swap_mode=none consistency=False master_addr=\$head_node backend=xccl gnn_outputs_path=${PWD}/gnn_outputs_poly_3 ckpt_dir=${PWD}/ckpt inference_dir=${PWD}/predictions verbose=True postprocess=True 2>&1 | tee inference.log" >> $RFILE
chmod u+x $RFILE

