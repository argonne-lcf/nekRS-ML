#!/bin/bash
set -e

#--------------------------------------
: ${QUEUE:="lustre_scaling"}
: ${NEKRS_GPU_MPI:=0}
: ${NEKRS_BACKEND:="dpcpp"}
: ${RANKS_PER_NODE:=12}
: ${CPU_BIND_LIST:="1:8:16:24:32:40:53:60:68:76:84:92"}
: ${OCCA_DPCPP_COMPILER_FLAGS:="-O3 -fsycl -fsycl-targets=intel_gpu_pvc -ftarget-register-alloc-mode=pvc:auto -fma"}
: ${ONEAPI_SDK:=""}
: ${FRAMEWORKS_MODULE:="frameworks"}
: ${VENV_PATH:=""}
#--------------------------------------

source $NEKRS_HOME/bin/nrsqsub_utils
setup $# 1

TOTAL_RANKS=$(( nodes * RANKS_PER_NODE ))
chk_case $TOTAL_RANKS

#--------------------------------------
# Generate the run script
RFILE=run.sh
echo "#!/bin/bash" > $RFILE

echo "export TZ='/usr/share/zoneinfo/US/Central'" >> $RFILE

echo -e "\necho Jobid: \$PBS_JOBID" >>$RFILE
echo "echo Running on host \`hostname\`" >>$RFILE
echo "echo Running on nodes \`cat \$PBS_NODEFILE\`" >>$RFILE

echo "module restore" >> $RFILE
echo "module load ${FRAMEWORKS_MODULE}" >> $RFILE
echo "source ${VENV_PATH}" >> $RFILE
echo "module list" >> $RFILE

echo -e "\nexport NEKRS_HOME=$NEKRS_HOME" >>$RFILE
#echo "export NEKRS_GPU_MPI=$NEKRS_GPU_MPI" >>$RFILE
#echo "export MPICH_GPU_SUPPORT_ENABLED=$NEKRS_GPU_MPI" >> $RFILE
echo "export OCCA_DPCPP_COMPILER_FLAGS=\"$OCCA_DPCPP_COMPILER_FLAGS\"" >> $RFILE
echo "export FI_CXI_RX_MATCH_MODE=hybrid" >> $RFILE # required by parRSB

# Temporary workaround while waiting on bugfix in runtime
echo "export UR_L0_USE_COPY_ENGINE=0" >> $RFILE

echo -e "\n# Precompilation" >>$RFILE
CMD_build="mpiexec -n ${RANKS_PER_NODE} -ppn ${RANKS_PER_NODE} --cpu-bind list:${CPU_BIND_LIST} -- $bin --setup ${case} --backend ${NEKRS_BACKEND} $extra_args --build-only ${RANKS_PER_NODE}"
add_build_CMD "$RFILE" "$CMD_build" "$TOTAL_RANKS"

echo -e "\n# Run nekRS" >>$RFILE
echo "#mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} -- $NEKRS_HOME/bin/nekrs --setup ${case} --backend ${NEKRS_BACKEND} 2>&1 | tee nekrs.log" >> $RFILE
echo "mpiexec -n 1 -ppn 1 --cpu-bind=list:${CPU_BIND_LIST} -- $NEKRS_HOME/bin/nekrs --setup ${case} --backend ${NEKRS_BACKEND} 2>&1 | tee nekrs.log" >> $RFILE

echo -e "\n# Generate the halo_info, edge_weights and node_degree files" >>$RFILE
echo "#mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/create_halo_info_par.py --POLY 3 --PATH ./gnn_outputs_poly_3" >> $RFILE
echo "mpiexec -n 1 -ppn 1 --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/create_halo_info_par.py --POLY 3 --PATH ./gnn_outputs_poly_3" >> $RFILE

echo -e "\n# Train model" >>$RFILE
echo "head_node=\`head -1 \$PBS_NODEFILE | cut -d'.' -f1\`" >>$RFILE
echo "#mpiexec -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/main.py halo_swap_mode=all_to_all_opt phase1_steps=5 backend=xccl gnn_outputs_path=${PWD}/gnn_outputs_poly_3 verbose=True 2>&1 | tee train.log" >> $RFILE
echo "mpiexec -n 1 -ppn 1 --cpu-bind=list:${CPU_BIND_LIST} python ${NEKRS_HOME}/3rd_party/dist-dgn/main.py master_addr=\$head_node halo_swap_mode=all_to_all_opt phase1_steps=5 backend=xccl gnn_outputs_path=${PWD}/gnn_outputs_poly_3 2>&1 | tee train.log" >> $RFILE

echo -e "\n# Perform inference and generate files for visualization" >>$RFILE
echo "#python ${NEKRS_HOME}/3rd_party/sr-gnn/postprocess.py --model_path $PWD/saved_models/gnn_3_7_132_128_3_2_6_True.tar --case_path $PWD --output_name ${case} --target_snap_list turbChannel_p70.f00000 --input_snap_list turbChannel_p10.f00000 --target_poly_order 7 --input_poly_order 1 --n_element_neighbors 12" >> $RFILE
echo "#cd predictions/gnn_3_7_132_128_3_2_6_True" >> $RFILE
echo "#$NEKRS_HOME/bin/nrsvis turbChannel_pred" >> $RFILE
echo "#$NEKRS_HOME/bin/nrsvis turbChannel_error" >> $RFILE
echo "#cd ../../" >> $RFILE
chmod u+x $RFILE

