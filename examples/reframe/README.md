# ReFrame Tests

## Organization

The ReFrame configuration is in `sites.py`. Currently, the `sites.py` only contains
configuration for ALCF Aurora. We define two partitions for Aurora: `aurora:compute`
and `aurora:login`. You can pass this configuration file into reframe using the `-C`
flag and use the `--system` flag to select the partition you like. You can edit the
`sites.py` if you would like to make changes to the existing Aurora parameters.

The `core.py` contains thin wrappers over `reframe.CompileOnlyRegressionTest` and
`reframe.RunOnlyRegressionTest` which are named `CompileOnlyTest` and `RunOnlyTest`
respectively. These classes are used based on the type of test we want to execute.

## Prerequisites

This repository contains the [ReFrame](https://reframe-hpc.readthedocs.io/en/stable/)
implementations of various tests, regression tests, and performance benchmarks.

In order to run tests, you first need to install [uv](https://docs.astral.sh/uv/getting-started/installation/).
Installation instructions can be found [here](https://docs.astral.sh/uv/getting-started/installation/).

## How to run the tests

```sh
>./run.sh -h
Usage: run.sh [options]

Options:
  --system,      -s   <SYSTEM>         Set the system name (Default: aurora:compute)
  --prefix,      -p   <PREFIX>         Set the installation prefix (Default: ${PWD}/test_data)
  --queue,       -q   <QUEUE>          Set the job queue (Default: prod)
  --project,     -prj <PROJECT>        Set the project name (Default: Performance)
  --filesystems, -f   <FS>             Set the filesystem (Default: home)
  --tag,         -t   <TAG>            Run the tests with tag TAG (Default: ml)
  --list-tags    -l                    List the available test tags
  --build,       -b                    Build the dependencies from scratch (do not reuse)
  --clean,       -c                    Clean the temporary files generated by Python etc.
  --help,        -h                    Show this help message and exit

Examples:
  To run all the tests:
    ./run.sh -b
  or
    ./run.sh -b -t all
  To run only the ml tests (which include online, offline, etc.):
    ./run.sh -t ml -b
  To run the all the offline tests:
    ./run.sh -t offline -b
  To run all the tests based on tgv:
    ./run.sh -t tgv -b
  Please note that the "-b (--build)" parameter is only required in the first run of  each tag.
  You can pass --list-tags or -l to list all the test tags:
    ./run.sh -l
```

## Output artifacts

A successful run will produce test artifacts in `${PWD}/test_data` directory. For
example, if you ran the `tgv_offline` test with the following command (on Aurora):
```sh
./run.sh -t tgv_offline -b
```

You will see the following directory structure:
```
test_data/
├── output
│   └── 20251024T184920+0000
│       ├── aurora
│       │   └── compute
│       │       └── PrgEnv-Aurora
│       │           ├── NekRSBuild_6b4abd03
│       │           ├── NekRSTGVOffline
│       │           ├── NekRSTGVOfflineCoarseMesh
│       │           └── NekRSTGVOfflineTraj
│       ├── rfm-dwsf8i1k.log
│       └── rfm-n430vm9z.log
├── reports
│   └── report_0.json
└── stage
    └── 20251024T184920+0000
        └── aurora
            └── compute
                └── PrgEnv-Aurora
                    ├── _env
                    ├── NekRSBuild_6b4abd03
                    ├── NekRSTGVOffline
                    ├── NekRSTGVOfflineCoarseMesh
                    └── NekRSTGVOfflineTraj

20 directories, 3 files
```

You will see two log files (`rfm-*.log`) under `${PREFIX}/output/<timestamp>/`.
One corresponds to a log level of `info` and the other is with the log level of
`debug2` (these levels are defined by reframe). Output artifacts produced by the
tests themselves will be under `${PREFIX}/output/<timestamp>/<system>/<PrgEnv>/<test>`.
The test reports are in`${PREFIX}/reports/report_<session_id>.json`. The staging
files of the tests can be found under `${PREFIX}/stage/<timestamp>`.
