# ReFrame Tests

## Organization

The ReFrame configuration is in `sites.py`. Currently, the `sites.py` only contains
site configurations for ALCF Aurora and ALCF Polaris. You can edit the `sites.py` if
you would like to make changes to the existing site parameters.

The `core.py` contains thin wrappers over `reframe.CompileOnlyRegressionTest` and
`reframe.RunOnlyRegressionTest` which are named `CompileOnlyTest` and `RunOnlyTest`
respectively. These classes are used based on the type of test we want to execute.

## Prerequisites

This repository contains the [ReFrame](https://reframe-hpc.readthedocs.io/en/stable/)
implementations of various tests, regression tests, and performance benchmarks.

In order to run tests, you first need to install [uv](https://docs.astral.sh/uv/) on
the target system with

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

More installation instructions can be found [here](https://docs.astral.sh/uv/getting-started/installation/).

## Running Tests

From a login/compute node on the target system, use the `run.sh` script to launch the
tests. To see the full list of parameters accepted by the script, execute:

```sh
>./run.sh -h
Usage: run.sh [options]

Options:
  --system,      -s   <SYSTEM>         Set the system name (Default: aurora:compute)
  --prefix,      -p   <PREFIX>         Set the installation prefix (Default: ${PWD}/test_data)
  --queue,       -q   <QUEUE>          Set the job queue (Default: prod)
  --project,     -prj <PROJECT>        Set the project name (Default: datascience)
  --filesystems, -f   <FS>             Set the filesystem (Default: home)
  --tag,         -t   <TAG>            Run the tests with tag TAG (Default: ml)
  --list-tags    -l                    List the available test tags
  --build,       -b                    Build the dependencies from scratch (do not reuse)
  --clean,       -c                    Clean the temporary files generated by Python etc.
  --help,        -h                    Show this help message and exit
```

Examples:
```
  To run all the tests:
    ./run.sh -t all -b
  To run all the offline tests:
    ./run.sh -t offline -b
  To run all the online tests:
    ./run.sh -t online -b
  To run all the tests based on tgv:
    ./run.sh -t tgv -b
  Please note that "-b" parameter is only required in the first run of each tag.
  You can pass --list-tags or -l to list all the test tags:
    ./run.sh -l
```

Default parameters in the `run.sh` are set for ALCF Aurora. You can use the options
provided by `run.sh` to customize the tests for a given platform. For example, you
can use `--system/-s` to select the site, `--project/-prj` to select the correct
project, and `--queue/-q` to set the correct queue, etc.

You can use regex in the tag to filter the tests you would like to run. For example,
if you only want to run `tgv_offline` (without also running `tgv_offline_traj`,
etc.), you can use the following tag:

```sh
./run.sh -b -t tgv_offline$
```

### Aurora

We define two partitions for Aurora: `aurora:compute` and `aurora:login`. Below is
an example which runs the `offline` tests on Aurora:

```sh
./run.sh -b -t offline
```

### Polaris

We define two partitions for Polaris: `polaris:compute` and `polaris:login`. Below is
an example which runs the `offline` tests on Polaris:

```sh
./run.sh -b -t offline -s polaris:compute -q debug
```

## Output Artifacts

A successful run will show that all tests passed and produce test artifacts in
`${PWD}/test_data` directory. For example, if you ran the `tgv_offline` test with the
following command (on Aurora):
```sh
./run.sh -t tgv_offline -b -prj <PROJECT>
```

You will see the following output:
```
[==========] Running 5 check(s)
[==========] Started on Tue Nov  4 22:45:24 2025+0000

[----------] start processing checks
[ RUN      ] SmartRedisBuild ~aurora:compute+PrgEnv-Aurora /30e810d8 @aurora:compute+PrgEnv-Aurora
[       OK ] (1/5) SmartRedisBuild ~aurora:compute+PrgEnv-Aurora /30e810d8 @aurora:compute+PrgEnv-Aurora
[ RUN      ] NekRSBuild ~aurora:compute+PrgEnv-Aurora /6b4abd03 @aurora:compute+PrgEnv-Aurora
[       OK ] (2/5) NekRSBuild ~aurora:compute+PrgEnv-Aurora /6b4abd03 @aurora:compute+PrgEnv-Aurora
[ RUN      ] NekRSTGVOffline %num_nodes=1 %ranks_per_node=2 /5e38581e @aurora:compute+PrgEnv-Aurora
[ RUN      ] NekRSTGVOfflineCoarseMesh %num_nodes=1 %ranks_per_node=2 /0dc759b6 @aurora:compute+PrgEnv-Aurora
[ RUN      ] NekRSTGVOfflineTraj %num_nodes=1 %ranks_per_node=4 /67a9fb19 @aurora:compute+PrgEnv-Aurora
[       OK ] (3/5) NekRSTGVOfflineTraj %num_nodes=1 %ranks_per_node=4 /67a9fb19 @aurora:compute+PrgEnv-Aurora
[       OK ] (4/5) NekRSTGVOffline %num_nodes=1 %ranks_per_node=2 /5e38581e @aurora:compute+PrgEnv-Aurora
[       OK ] (5/5) NekRSTGVOfflineCoarseMesh %num_nodes=1 %ranks_per_node=2 /0dc759b6 @aurora:compute+PrgEnv-Aurora
[----------] all spawned checks have finished

[  PASSED  ] Ran 5/5 test case(s) from 5 check(s) (0 failure(s), 0 skipped, 0 aborted)
```

And the following directory structure (run `tree -L 5 ${PWD}/test_data/`):
```
test_data/
├── output
│   └── 20251024T184920+0000
│       ├── aurora
│       │   └── compute
│       │       └── PrgEnv-Aurora
│       │           ├── NekRSBuild_6b4abd03
│       │           ├── NekRSTGVOffline
│       │           ├── NekRSTGVOfflineCoarseMesh
│       │           └── NekRSTGVOfflineTraj
│       ├── rfm-dwsf8i1k.log
│       └── rfm-n430vm9z.log
├── reports
│   └── report_0.json
└── stage
    └── 20251024T184920+0000
        └── aurora
            └── compute
                └── PrgEnv-Aurora
                    ├── _env
                    ├── NekRSBuild_6b4abd03
                    ├── NekRSTGVOffline
                    ├── NekRSTGVOfflineCoarseMesh
                    └── NekRSTGVOfflineTraj

20 directories, 3 files
```

You will see two log files (`rfm-*.log`) under `${PWD}/test_data/output/<timestamp>/`.
One corresponds to a log level of `info` and the other is with the log level of
`debug2` (these levels are defined by ReFrame). Output artifacts produced by the
tests themselves will be under `${PWD}/test_data/output/<timestamp>/<system>/<PrgEnv>/<test>`.
The test reports are in`${PWD}/test_data/reports/report_<session_id>.json`. The
staging files of the tests can be found under `${PWD}/test_data/stage/<timestamp>`.

## Common Issues

* Make sure the correct project and system parameters are being passed to the test
  run script.
* If you see an error similar to the following, it means that the ReFrame couldn't
  find a report to restore the session you asked for. In that case, pass the `-b`
  flag to the `run.sh` to generate everything from scratch.
  ```
  FileNotFoundError: [Errno 2] No such file or directory: '/home/thilina/gnn/nekrs_ml.git/examples/reframe/test_data/reports'
  ```
